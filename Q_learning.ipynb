{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class environment:\n",
    "    def __init__(self, grid_height, grid_width):\n",
    "        \"\"\"\n",
    "        The map is initialized with height and width are variable of your choice\n",
    "        start: List of location where you step in, you get to the corresponding location in list 'end'\n",
    "                For example:\n",
    "                    if you step in location start[3] then you get to new location end[3] then obtain the reward value of reward[3]\n",
    "        reward: List of reward value where you move from a location in 'start' list to the corresponding location in 'end' list\n",
    "        \"\"\"\n",
    "        self.height = grid_height\n",
    "        self.width = grid_width\n",
    "        self.start = []\n",
    "        self.end = []\n",
    "        self.reward = []\n",
    "        self.map = np.array([i for i in range(grid_height * grid_width)])\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "\n",
    "    def get_map(self):\n",
    "        print(self.map.reshape([self.width, self.height]))\n",
    "\n",
    "    def get_num_state(self):\n",
    "        return self.height * self.width\n",
    "\n",
    "    def map_Designate(self, start_cell, end_cell, reward):\n",
    "        self.start.append(start_cell)\n",
    "        self.end.append(end_cell)\n",
    "        self.reward.append(reward)\n",
    "\n",
    "    def get_Observation(self, location, action):\n",
    "        # Default reward = 0\n",
    "        reward = 0\n",
    "        new_location = 0\n",
    "        # Action: UP: 0, DOWN: 1, LEFT: 2, RIGHT: 3\n",
    "        # Actions that get the agent out of the map, result in no change at all\n",
    "        if action == 0:  # UP\n",
    "            if location - self.width < 0:\n",
    "                new_location = location\n",
    "            else:\n",
    "                new_location = location - self.width\n",
    "\n",
    "        elif action == 1:  # DOWN\n",
    "            if location + self.width > self.height * self.width - 1:\n",
    "                new_location = location\n",
    "            else:\n",
    "                new_location = location + self.width\n",
    "\n",
    "        elif action == 2:  # LEFT\n",
    "            if location % self.width == 0:\n",
    "                new_location = location\n",
    "            else:\n",
    "                new_location = location - 1\n",
    "\n",
    "        elif action == 3:  # RIGHT\n",
    "            if (location + 1) % self.width == 0:\n",
    "                new_location = location\n",
    "            else:\n",
    "                new_location = location + 1\n",
    "\n",
    "        # If the agent is at special locations, immediately moves to corresponding destinations, gain reward\n",
    "        if new_location in self.start:\n",
    "            idx = self.start.index(new_location)\n",
    "            new_location = self.end[idx]\n",
    "            reward = self.reward[idx]\n",
    "\n",
    "        return new_location, self.action_space, reward\n",
    "\n",
    "\n",
    "class MABAgent:\n",
    "    def __init__(self, envir, init_location):\n",
    "        # Trace the reward\n",
    "        self.reward_trace = []\n",
    "        # initialize the first location\n",
    "        self.location_now = init_location\n",
    "        # TODO: implement other features to the agent so it can perform MAB algorithm\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "        self.value_table = {}  # format: {state: {action: [value, count]}}\n",
    "        for state in range(envir.width * envir.height):\n",
    "            self.value_table[state] = {}\n",
    "            for action in envir.action_space:\n",
    "                self.value_table[state][action] = [0, 0]\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return np.sum(self.reward_trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7]\n",
      " [ 8  9 10 11 12 13 14 15]\n",
      " [16 17 18 19 20 21 22 23]\n",
      " [24 25 26 27 28 29 30 31]\n",
      " [32 33 34 35 36 37 38 39]\n",
      " [40 41 42 43 44 45 46 47]\n",
      " [48 49 50 51 52 53 54 55]\n",
      " [56 57 58 59 60 61 62 63]]\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "Envir = environment(8,8)\n",
    "Envir.map_Designate(17,56,-15)\n",
    "Envir.map_Designate(18,56,-15)\n",
    "Envir.map_Designate(19,56,-15)\n",
    "Envir.map_Designate(21,56,-15)\n",
    "Envir.map_Designate(25,56,-15)\n",
    "Envir.map_Designate(33,56,-15)\n",
    "Envir.map_Designate(41,56,-15)\n",
    "Envir.map_Designate(42,56,-15)\n",
    "Envir.map_Designate(43,56,-15)\n",
    "Envir.map_Designate(46,56,-15)\n",
    "Envir.map_Designate(47,56,-15)\n",
    "Envir.map_Designate(47,56,-15)\n",
    "Envir.map_Designate(15,56,+15)\n",
    "Envir.map_Designate(1,10,+5)\n",
    "Envir.map_Designate(26,56,+20)\n",
    "\n",
    "Envir.get_map()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
